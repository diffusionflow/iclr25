<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://diffusionflow.github.io/iclr25/feed.xml" rel="self" type="application/atom+xml"/><link href="https://diffusionflow.github.io/iclr25/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-23T18:11:50+08:00</updated><id>https://diffusionflow.github.io/iclr25/feed.xml</id><title type="html">ICLR Blogposts 2025</title><subtitle>Home to the 2025 ICLR Blogposts track </subtitle><entry><title type="html">Diffusion Models and Gaussian Flow Matching: Two Sides of the Same Coin</title><link href="https://diffusionflow.github.io/iclr25/blog/distill-example-pre-kevin/" rel="alternate" type="text/html" title="Diffusion Models and Gaussian Flow Matching: Two Sides of the Same Coin"/><published>2025-11-12T00:00:00+08:00</published><updated>2025-11-12T00:00:00+08:00</updated><id>https://diffusionflow.github.io/iclr25/blog/distill-example-pre-kevin</id><content type="html" xml:base="https://diffusionflow.github.io/iclr25/blog/distill-example-pre-kevin/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/twotrees-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/twotrees-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/twotrees-1400.webp"/> <img src="/iclr25/assets/img/2025-04-28-distill-example/twotrees.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Flow matching is gaining popularity recently, due to its simplicity in formulation and “straightness” in the sampling trajectories. A common question one hears nowadays is:</p> <p align="center"><i>"Does this diffusion technique also work with flow matching?"</i></p> <p>What exactly are the differences between these two approaches? As we will see, diffusion modelling and Gaussian flow matching are the same. So the answer to this question is “yes”, unless the matching is not to a Gaussian.</p> <p>To give an example, you may assume that flow matching sampling has to be deterministic. However, you have trained a general denoiser: stochastic or deterministic sampling, it’s up to you!</p> <p>In this blog post, we take the most commonly<d-footnote>We focus on Gaussian flow matching with the optimal transport flow path.</d-footnote> used flow matching case <d-cite key="lipman2022flow"></d-cite>, also very related to <d-cite key="liu2022flow,albergo2023stochastic"></d-cite>. Our purpose is not to downweigh the importance of either framework. In fact, both diffusion model and flow matching frameworks are important and derived from distinct theoretical perspectives. It is even more encouraging that they lead to the same algorithm in practice. The goal of this post is to make the practitioner feel comfortable to use the two frameworks interchangeably and understand the actual degrees of freedom we have when tuning the algorithm (no matter how we name it).</p> <h2 id="overview">Overview</h2> <p>We start by a quick overview of the two frameworks. We compare them from a high level and will see that the <em>processes</em> are the same. </p> <h3 id="diffusion-models">Diffusion models</h3> <p>A diffusion process gradually destroys an observed data \(\bf{x}\) over time \(t\), by mixing the data with Gaussian noise. Summing up this noise over time gives: \(\begin{equation} {\bf z}_t = \alpha_t {\bf x} + \sigma_t {\boldsymbol \epsilon}, \;\mathrm{where} \; {\boldsymbol \epsilon} \sim \mathcal{N}(0, {\bf I}). \label{eq:forward} \end{equation}\) \(\alpha_t\) and \(\sigma_t\) define the <strong>noise schedule</strong>. A commonly used one is the variance-preserving schedule (\(\alpha_t^2 + \sigma_t^2 = 1\)). A useful notation is the log signal-to-noise ratio \(\lambda_t = \log(\alpha_t^2 / \sigma_t^2)\), which decreases as \(t\) increases from \(0\) (clean data) to \(1\) (Gaussian noise).</p> <p>To generate new samples, we can “reverse” the forward process gradually: Initialize the sample from Gaussian at the highest noise level. Given the sample \({\bf z}_t\) at time step \(t\), we predict what the clean sample might look like with a neural network \(\hat{\bf x} = \hat{\bf x}({\bf z}_t; t)\) (a.k.a. denoiser model), and then we project it back to a lower noise level \(s\) with the same forward transformation:</p> <p>\(\begin{eqnarray} {\bf z}_{s} &amp;=&amp; \alpha_{s} \hat{\bf x} + \sigma_{s} \hat{\boldsymbol \epsilon},\\ \end{eqnarray}\) where \(\hat{\boldsymbol \epsilon} = ({\bf z}_t - \alpha_t \hat{\bf x}) / \sigma_t\). We can also parametrize \(\hat{\boldsymbol \epsilon}\) with a neural network. We keep alternating between predicting the clean data, and projecting it back to a lower noise level until we get the clean sample. This is the DDIM sampler <d-cite key="song2020denoising"></d-cite>. The randomness of samples only comes from the initial Gaussian sample, and the entire reverse process is deterministic. We will discuss the stochastic samplers later.</p> <h3 id="flow-matching">Flow matching</h3> <p>Flow Matching provides another perspective of the forward process: we view it as a direct interpolation between the data \({\bf x}\) and the Gaussian noise \(\boldsymbol \epsilon\), not adding noise gradually. In the more general case, \(\boldsymbol \epsilon\) can also be sampled from an arbitrary distribution. The forward process should look familiar to the reader, and is defined as: \(\begin{eqnarray} {\bf z}_t = t {\bf x} + (1-t) {\boldsymbol \epsilon}.\\ \end{eqnarray}\)</p> <p>It corresponds to the diffusion forward process with \(\alpha_t = t, \sigma_t = 1-t\). The flow from time \(s\) to \(t\) (\(s &lt; t\)) can be expressed as \({\bf z}_t = {\bf z}_{s} + {\bf u} (t - s)\), where \({\bf u} = {\bf x} - {\bf \epsilon}\) is the “velocity”, “flow”, or “vector field”. For sampling, we reverse the time and replace the vector field with our best guess of the clean data at time step \(t\), since we do not have access to it during sampling:</p> <p>\(\begin{eqnarray} {\bf z}_{s} = {\bf z}_t + \hat{\bf u}(s - t).\\ \label{eq:flow_update} \end{eqnarray}\) \(\hat{\bf u} = \hat{\bf u}({\bf z}_t; t) = \hat{\bf x} - \hat{\boldsymbol \epsilon}\) can be parametrized by a neural network.</p> <p>So far we can already sense the similar flavors of the two frameworks:</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> <p>1. <strong>Same forward process</strong>: assume that one end of flow matching is Gaussian, and the noise schedule of diffusion models is in a particular form. </p> <p style="margin: 0;">2. <strong>"Similar" sampling processes</strong>: both follow an iterative update that involves a guess of the clean data at the current time step. (Spoiler: we will show they are exactly the same!)</p> </div> <h2 id="sampling-and-straightness-misnomer">Sampling and Straightness Misnomer</h2> <p>A common thought is that the two frameworks are different in sampling: Flow matching is deterministic and with “straight” paths, while diffusion model sampling is stochastic and with curved paths. Let’s discuss it first.</p> <p>We will focus on deterministic sampling first which is simpler. Imagine you want to use your trained denoiser model to transform random noise into a datapoint. Recall that the DDIM update is given by \({\bf z}_{s} = \alpha_{s} \hat{\bf x} + \sigma_{s} \hat{\boldsymbol \epsilon}\). Interestingly, rearranging terms it can be expressed in the following formulation, with respect to several network outputs and reparametrizations:</p> \[\begin{equation} \tilde{\bf z}_{s} = \tilde{\bf z}_{t} + \mathrm{Network \; output} \cdot (\eta_s - \eta_t) \\ \end{equation}\] <table> <thead> <tr> <th style="text-align: left">Network Output</th> <th style="text-align: right">Reparametrization</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">\(\hat{\bf x}\)-prediction</td> <td style="text-align: right">\(\tilde{\bf z}_t = {\bf z}_t / \sigma_t\) and \(\eta_t = {\alpha_t}/{\sigma_t}\)</td> </tr> <tr> <td style="text-align: left">\(\hat{\boldsymbol \epsilon}\)-prediction</td> <td style="text-align: right">\(\tilde{\bf z}_t = {\bf z}_t / \alpha_t\) and \(\eta_t = {\sigma_t}/{\alpha_t}\)</td> </tr> <tr> <td style="text-align: left">\(\hat{\bf u}\)-flow matching vector field</td> <td style="text-align: right">\(\tilde{\bf z}_t = {\bf z}_t/(\alpha_t + \sigma_t)\) and \(\eta_t = {\alpha_t}/(\alpha_t + \sigma_t)\)</td> </tr> </tbody> </table> <p>Recall the flow matching update in Equation (4), look similar? With \(\hat{\bf u}\) prediction and \(\alpha_t = t\), \(\sigma_t = 1- t\), we have \(\tilde{\bf z}_t = {\bf z}_t\) and \(\eta_t = t\), so that it recovers the flow matching update! More formally, the flow matching update can be considered the Euler integration of the underlying sampling ODE <d-footnote> That is, $\mathrm{d}\tilde{\bf z}_t = \mathrm{[Network \; output]}\cdot\mathrm{d}\eta_t$</d-footnote>, and</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> <p align="center" style="margin: 0;"><em>Diffusion with DDIM sampling == Flow matching sampling (Euler).</em></p> </div> <p>Other traits about the DDIM sampler:</p> <ol> <li> <p>DDIM sampler <em>analyically</em> integrates the underlying sampling ODE if the network output is a <em>constant</em> over time. Of course the network prediction is not constant, but it means the inaccuracy of DDIM sampler only comes from approximating the intractable integral of the network output <d-footnote>not from additional linear term of ${\bf z}_t$ as in the Euler sampler of probability flow ODE <d-cite key="song2020score"></d-cite></d-footnote>. This holds for all three network outputs.</p> </li> <li> <p>DDIM update and final samples are invariant to a linear scaling applied to the noise schedule, as a scaling does not affect $\tilde{\bf z}_t$ and $\eta_t$.</p> </li> </ol> <p>Skeptical about 2? Check it out for yourself below: We consider several settings between the flow matching (FM) schedule and a variance-preserving (VP) schedule by changing a linear scaling. DDIM always gives the same final samples regardless of the scaling of the schedule, which is also the same as the flow matching sampler. The paths bend in different ways as \({\bf z}_t\) (not \(\tilde{\bf z}_t\)) is scale-dependent along the path. For the Euler sampler of diffusion probabilty flow ODE introduced in <d-cite key="song2020score"></d-cite>, the scaling makes a true difference: Both the paths and the final samples change.</p> <div class="l-page"> <iframe src="/iclr25/assets/html/2025-04-28-distill-example/interactive_alpha_sigma.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p align="center"><i>"Flow matching paths are straight, whereas diffusion paths are curved."</i></p> <p>Wait? The flow matching schedule is said to result in straighter paths, but in the above figure its sampling trajectories look <em>curved</em> and the VP paths look <em>straight</em>.</p> <p>So why is the flow matching parameterization said to result in straighter sampling paths? If the model would be perfectly confident about the data point it is moving to, the path from noise to data will be a straight line with the flow matching schedule. Straight line ODEs would be great because it means that there is no integration error whatsoever. Unfortunately, the predictions are not for a single point. Instead they average over a larger distribution. </p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> <p align="center" style="margin: 0;"><em>Straight to a point != Straight to a distribution.</em></p> </div> <p>In fact, in the interactive graph below you can change the variance of a simple Guassian data distribution. Note how the variance preserving schedule is the best for wide distributions while the flow matching schedule works well for narrow distributions.</p> <div class="l-page"> <iframe src="/iclr25/assets/html/2025-04-28-distill-example/interactive_vp_vs_flow.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p>Finding such straight paths for real-life datasets like images is of course much less straightforward. But the conclusion remains the same: The optimal integration method depends on the data and the models prediction.</p> <p>Two important takeaways from determinstic sampling:</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> <p>1. DDIM is equivalent to the flow matching sampling, and is invariant to a linear scaling to the noise schedule. </p> <p style="margin: 0;">2. Flow matching schedule is only straight for a model predicting a single point. For realistic distributions other interpolations can give straighter paths.</p> </div> <h2 id="training-weighting-output-schedule">Training (weighting, output, schedule)</h2> <p>Diffusion models <d-cite key="kingma2024understanding"></d-cite> are trained by estimating \(\hat{\bf x} = \hat{\bf x}({\bf z}_t; t)\) with a neural net. In practice, one chooses a linear combination of \(\hat{\bf x}\) and \({\bf z}_t\) for stability reasons. Learning the model is done by minimizing a weighted mean squared error (MSE) loss: \(\begin{equation} \mathcal{L}(\mathbf{x}) = \mathbb{E}_{t \sim \mathcal{U}(0,1), \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})} \left[ \textcolor{green}{w(\lambda_t)} \cdot \frac{\mathrm{d}\lambda}{\mathrm{d}t} \cdot \lVert\hat{\bf x} - {\bf x}\rVert_2^2 \right], \end{equation}\) where \(\lambda_t\) is the log signal-to-noise ratio, and \(\textcolor{green}{w(\lambda_t)}\) is the <strong>weighting function</strong>, balancing the importance of the loss at different noise levels. The term \(\mathrm{d}\lambda / {\mathrm{d}t}\) in the training objective seems unnatural and in the literature is often merged with the weighting function. However, their separation helps <em>disentangle</em> the factors of noise schedule and weighting function clearly, and helps emphasize the more important weighting components.</p> <p>Flow matching also fits in the training objective, recall the conditional flow matching objective used by <d-cite key="lipman2022flow, liu2022flow"></d-cite> is</p> \[\begin{equation} \mathcal{L}_{\mathrm{CFM}}(\mathbf{x}) = \mathbb{E}_{t \sim \mathcal{U}(0,1), \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})} \left[ \lVert \hat{\bf u} - {\bf u} \rVert_2^2 \right] \end{equation}\] <p>Since \(\hat{\bf u}\) is also a linear combination of \(\hat{\bf x}\) and \({\bf z}_t\), the CFM training objective can be rewritten as mean squared error on \({\bf x}\) with a specific weighting.</p> <h3 id="whats-the-weight">What’s the weight?</h3> <p>The weighting is the most important part of the loss, it balances the importance of high frequency and low frequency components. <strong>(TODO, making a figure to illustrate weighting function versus frequency components.)</strong> This is important when modeling images, videos and audios, as certain high frequency components in those signals are not visible to human perception, and thus better not to waste model capacity on them. Viewing losses via their weighting, one can derive that:</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> <p align="center" style="margin: 0;"><em>Flow matching weighting == diffusion weighting of ${\bf v}$-MSE loss + cosine noise schedule.</em></p> </div> <p>That is, the flow matching training objective is the same as a commonly used setting in diffusion models! See Appendix D.2-3 in <d-cite key="kingma2024understanding"></d-cite> for a detailed derivation. Figure <strong>TODO</strong> plots several commonly used weighting functions in the literature.</p> <h3 id="network-output">Network output</h3> <p>Below we summarize several network outputs proposed in the literature, including a few of diffusion models and the one of flow matching. One may see the training objective defined with different network outputs in different papers. From the perspective of training objective, they all correspond to having some additional weighting in front of the \({\bf x}\)-MSE that can be absorbed in the weighting function.</p> <table> <thead> <tr> <th style="text-align: left">Network Output</th> <th style="text-align: center">Formulation</th> <th style="text-align: right">MSE on Network Output</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">\({\bf x}\)-prediction</td> <td style="text-align: center">\(\hat{\bf x}\)</td> <td style="text-align: right">\(\lVert\hat{\bf x} - {\bf x}\rVert_2^2\)</td> </tr> <tr> <td style="text-align: left">\({\boldsymbol \epsilon}\)-prediction</td> <td style="text-align: center">\(\hat{\boldsymbol \epsilon} = ({\bf z}_t - \alpha_t \hat{\bf x}) / \sigma_t\)</td> <td style="text-align: right">\(\lVert\hat{\boldsymbol{\epsilon}} - \boldsymbol{\epsilon}\rVert_2^2 = e^{\lambda} \lVert\hat{\bf x} - {\bf x}\rVert_2^2\)</td> </tr> <tr> <td style="text-align: left">\({\bf v}\)-prediction</td> <td style="text-align: center">\(\hat{\bf v} = \alpha_t \hat{\boldsymbol{\epsilon}} - \sigma_t \hat{\bf x}\)</td> <td style="text-align: right">\(\lVert\hat{\bf v} - {\bf v}\rVert_2^2 = \sigma_t^2(e^{-\lambda} + 1)^2 \lVert\hat{\bf x} - {\bf x}\rVert_2^2\)</td> </tr> <tr> <td style="text-align: left">\({\bf u}\)-flow matching vector field</td> <td style="text-align: center">\(\hat{\bf u} = \hat{\bf x} - \hat{\boldsymbol{\epsilon}}\)</td> <td style="text-align: right">\(\lVert\hat{\bf u} - {\bf u}\rVert_2^2 = (1 + e^{\lambda / 2})^2 \lVert\hat{\bf x} - {\bf x}\rVert_2^2\)</td> </tr> </tbody> </table> <p>In practice, however, the model output might make a difference. For example,</p> <ul> <li>\({\bf x}\)-prediction can be problematic at low noise levels, because small changes create a large loss under typical weightings. You can also see in the sampler that any error in \(\hat{\bf x}\) will get amplified in \(\hat{\boldsymbol \epsilon} = ({\bf z}_t - \alpha_t \hat{\bf x}) / \sigma_t\), as \(\sigma_t\) is close to 0.</li> <li>Following the similar reason, \({\boldsymbol \epsilon}\)-prediction is problematic at high noise levels, because \(\hat{\boldsymbol \epsilon}\) is not informative, and the error gets amplified in \(\hat{\bf x}\).</li> </ul> <p>Therefore, a heuristic is to choose a network output that is a combination of \({\bf x}\)- and \({\boldsymbol \epsilon}\)-prediction, which applies to the \({\bf v}\)-prediction and the flow matching vector field \({\bf u} = {\bf x} - {\bf \epsilon}\).</p> <h3 id="noise-schedule">Noise schedule</h3> <p>A few remarks about training noise schedule:</p> <ol> <li>All noise schedules can be normalized as a variance-preserving schedule, with a linear scaling of \({\bf z}_t\) and an unscaling at the network input. The key defining property of a noise schedule is the log signal-to-noise ratio \(\lambda_t\).</li> <li>The training loss is <em>invariant</em> to the training noise schedule, since the loss fuction can be rewritten as \(\mathcal{L}(\mathbf{x}) = \int_{\lambda_{\min}}^{\lambda_{\max}} w(\lambda) \mathbb{E}_{\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})} \left[ \|\hat{\boldsymbol{\epsilon}} - \boldsymbol{\epsilon}\|_2^2 \right] \, d\lambda\), which is only related to the endpoints but not the schedule of \(\lambda_t\). However, \(\lambda_t\) might still affect the variance of the Monte Carlo estimator of the training loss. A few heuristics have been proposed in the literature to automatically adjust the noise schedules over the training course. <a href="https://sander.ai/2024/06/14/noise-schedules.html#adaptive">Sander’s blog post</a> has a nice summary.</li> <li>one can choose completely different noise schedules for training and sampling, based on distinct heuristics: For training, it is desirable to have a noise schedule that minimizes the variance of the Monte Calor estimator, whereas for sampling the noise schedule is more related to the discretization error of the ODE / SDE sampling trajectories and the model curvature.</li> </ol> <p>In summary, we have the following conclusions for diffusion models / flow matching training:</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> <p>1. Weighting function <strong> is important for training</strong>. For perceptual signals, it balances the importance of different frequency components. It should be tuned based on data characteristics. </p> <p>2. Noise schedule <strong>is far less important to the training objective</strong> and affects the training efficiency.</p> <p style="margin: 0;">3. The network output proposed by flow matching nicely balances ${\bf x}$- and ${\epsilon}$-prediction, similar to ${\bf v}$-prediction.</p> </div> <h2 id="diving-deeper-into-samplers">Diving deeper into samplers</h2> <h3 id="reflow-operator">Reflow operator</h3> <p>The Reflow operation in Flow Matching connects noise and data points to sample in a straight line. One can obtain these data noise pairs by running a deterministic sampler from noise. A model can then be trained to directly predict the data given the noise avoiding the need for sampling. In the diffusion literature the same approach was the one of the first distillation techniques <d-cite key="luhman2021knowledge"></d-cite>.</p> <h3 id="deterministic-sampler-vs-stochastic-sampler">Deterministic sampler vs. stochastic sampler</h3> <p>So far we mainly cover the deterministic sampler of diffusion models or flow matching. An alternative is to use stochastic samplers such as the DDPM sampler <d-cite key="ho2020denoising"></d-cite>. The key is to realize that, the effect of a small step of DDIM update can be canceled out by a small step of forward diffusion update in distribution. To see why it is true, let’s take a look at a 2D example. Starting from the same mixture of Gaussians distribution, we either apply a reverse DDIM update, or a diffusion update:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/particle_movement.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/particle_movement.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/particle_movement.gif-1400.webp"/> <img src="/iclr25/assets/img/2025-04-28-distill-example/particle_movement.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>For each individual sample, the two updates are very different. The reverse DDIM update consistently drags every sample away from the modes of the distribution, while the diffusion update is purely random. However, aggregating all samples together, the distributions after the updates are the same, thereby validating our claim. That means we can run DDIM update with a large step then followed by a “renoising” step, which matches the effect of running DDIM update with a smaller step.</p> <p>In fact, performing one DDPM sampling step going from $\lambda_t$ to $\lambda_t + \Delta\lambda$ is exactly equivalent to performing one DDIM sampling step to $\lambda_t + 2\Delta\lambda$, and then renoising to $\lambda_t + \Delta\lambda$ by doing forward diffusion. DDPM thus reverses exactly half the progress made by DDIM in terms of the log signal-to-noise ratio. However, the fraction of the DDIM step to undo by renoising is a hyperparameter which we are free to choose, and which has been called the level of <em>churn</em> by <d-cite key="karras2022elucidating"></d-cite>. The effect of adding churn to our sampler is to diminish the effect on our final sample of our model predictions made early during sampling, and to increase the weight on later predictions. This is shown in the Figure below</p> <div class="l-page"> <iframe src="/iclr25/assets/html/2025-04-28-distill-example/churn.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p>Here we ran different samplers for 100 sampling steps using a cosine noise schedule and v-prediction <d-cite key="salimansprogressive"></d-cite>. Ignoring nonlinear interactions, the final sample produced by the sampler can be written as a weighted sum of predictions made during sampling and noise. The weights of these predictions are shown on the y-axis for different diffusion times shown on the x-axis. DDIM results in an equal weighting of v-predictions for this setting, as shown by Salimans &amp; Ho, whereas DDPM puts more emphasis on predictions made towards the end of sampling. Also see <d-cite key="lu2022dpm"></d-cite> for analytic expressions of these weights in the x and $\epsilon$ parameterizations.</p> <h2 id="from-diffusion-models-to-flow-matching-and-back">From Diffusion Models to Flow Matching and back</h2> <p>So far, we have shown that the equivalence of the flow matching sampler and the DDIM sampler. We have also shown that the weightings appearing in flow matching and diffusion models can all be expressed in a general framework by expressing them in terms of log-SNR. This should (hopefully!) have convinced you that the frameworks are identical. But how easily can you move from one framework to the other? Below, we derive <strong>exact formula</strong> to move from a diffusion model to a flow matching perspective and vice-versa.</p> <h3 id="diffusion-models-framework-hyperparameters">Diffusion Models framework hyperparameters</h3> <p>We have stated in the overview that “A diffusion process gradually destroys an observed data \(\bf{x}\) over time \(t\)”. But what is this gradual process? It can be fully described by the following evolution equation</p> \[\begin{equation} \mathrm{d} {\bf z}_t = f_t {\bf z}_t \mathrm{d} t + g_t \mathrm{d} {\bf z} , \end{equation}\] <p>where \(\mathrm{d} {\bf z}\) is an <em> infinitesimal Gaussian</em> <d-footnote>If you want to be fancy this is usually referred to as a Brownian motion in the literature. </d-footnote></p> <p>Looking at this representation, the free parameters are given by $f_t$ and $g_t$. From the diffusion model perspective, the generative process is given by the reverse of the forward process, whose formula is given by</p> \[\begin{equation} \mathrm{d} {\bf z}_t = \left( f_t {\bf z}_t - \frac{1+ \eta_t^2}{2}g_t^2 \nabla \log p_t({\bf z_t}) \right) \mathrm{d} t + \eta_t g_t \mathrm{d} {\bf z} , \end{equation}\] <p>where $\nabla \log p_t$ is the <em>score</em> of the forward process <d-footnote>This is why you might have noticed that some papers refer to diffusion models as "score-based generative models".</d-footnote></p> <p>Note that we have introduced an additional parameter $\eta_t$ which controls the amount of stochasticity at inference time. This is exactly the <em>churn</em> parameter introduced before. When discretizing the backward process we recover DDIM in the case $\eta_t = 0$ and DDPM in the case $\eta_t = 1$. So to summarize:</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> Diffusion model frameworks can be entirely determined by three hyperparameters <p>1. $f_t$ which controls how much we forget the original data in the forward process. </p> <p>2. $g_t$ which controls how much noise we input into the samples in the forward process. </p> <p style="margin: 0;">3. $\eta_t$ which controls the amount of stochasticity at inference time. </p> </div> <h3 id="flow-matching-framework-hyperparameters">Flow Matching framework hyperparameters</h3> <p>Now let’s turn to flow matching and its degrees of freedom. We consider a slightly more general setting than in the overview and introduce the following interpolation</p> \[\begin{equation} {\bf z}_t = \alpha_t {\bf x} + \sigma_t {\bf z} . \end{equation}\] <p>This is a specific case of the general <em>stochastic interpolation</em><d-cite key="liu2022flow,albergo2023stochastic"></d-cite>. In that case, the free parameters are given by $\alpha_t$ and $\sigma_t$. From the flow matching perspective, the generative process is by the following trajectory</p> \[\begin{equation} \mathrm{d} {\bf z}_t = (v_t({\bf z_t}) - \varepsilon_t^2 \nabla \log p_t({\bf z_t})) \mathrm{d} t + \varepsilon_t \mathrm{d} {\bf z} . \end{equation}\] <p>Note that we have introduced an additional parameter $\varepsilon_t$ which controls the amount of stochasticity at inference time.</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> Flow matching frameworks can be entirely determined by three hyperparameters <p>1. $\alpha_t$ which controls the data component in the interpolation. </p> <p>2. $\sigma_t$ which controls the noise component in the interpolation. </p> <p style="margin: 0;">3. $\varepsilon_t$ which controls the amount of stochasticity at inference time. </p> </div> <h3 id="equivalence-of-the-points-of-view">Equivalence of the points of view</h3> <p>Despite their clear similarities it is not immediately clear how to link the diffusion model framework and the flow matching one. Below, we provide formulae which give a one-to-one mapping between the two frameworks. In short:</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> Diffusion model and flow matching are just one change of variable away! </div> <p>Given a diffusion model framework, i.e. hyperparameters $f_t, g_t, \eta_t$ one can define</p> \[\begin{equation} \alpha_t = \exp\left(\int_0^t f_s \mathrm{d}s\right) , \qquad \sigma_t = \left(\int_0^t g_s^2 \exp\left(-2\int_0^s f_u \mathrm{d}u\right) \mathrm{d} s\right)^{1/2} , \qquad \varepsilon_t = \eta_t g_t . \end{equation}\] <p>Doing so, the noising process induced by the flow matching and the diffusion framework as well as the generative trajectories! Similarly, given a flow matching framework, i.e. hyperparameters $\alpha_t, \sigma_t, \varepsilon_t$ one can define</p> \[f_t = \partial_t \log(\alpha_t) , \qquad g_t = 2 \alpha_t \sigma_t \partial_t (\sigma_t / \alpha_t) , \qquad \eta_t = \varepsilon_t / (2 \alpha_t \sigma_t \partial_t (\sigma_t / \alpha_t)) .\] <p>We have the similar conclusion, that under this transformation, diffusion models and flow matching frameworks coincide. To sum up, leaving aside training issues and the choice of the sampler, there is no fundamental differences between the two approaches.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Flow matching and diffusion models are two popular frameworks in generative modeling. Despite seeming similar, there is general confusion in the community about their exact connection. In this post we aim to clear up this confusion and show that diffusion model and Gaussian flow matching are essentially the same: Different model specifications lead to different noise schedules and loss weighting but correspond to the same generative model. That's great news, it means that you can use the two frameworks interchangeably]]></summary></entry><entry><title type="html">Diffusion Models and Gaussian Flow Matching: Two Sides of the Same Coin</title><link href="https://diffusionflow.github.io/iclr25/blog/distill-example/" rel="alternate" type="text/html" title="Diffusion Models and Gaussian Flow Matching: Two Sides of the Same Coin"/><published>2025-11-12T00:00:00+08:00</published><updated>2025-11-12T00:00:00+08:00</updated><id>https://diffusionflow.github.io/iclr25/blog/distill-example</id><content type="html" xml:base="https://diffusionflow.github.io/iclr25/blog/distill-example/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/twotrees-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/twotrees-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/twotrees-1400.webp"/> <img src="/iclr25/assets/img/2025-04-28-distill-example/twotrees.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Flow matching is gaining popularity recently, due to the simplicity of its formulation and the “straightness” of its induced sampling trajectories. This raises the commonly asked question:</p> <p align="center"><i>"Which is better, diffusion or flow matching?"</i></p> <p>As we will see, diffusion models and flow matching are <em>equivalent</em> (for the common special case that the source distribution used with flow matching corresponds to a Gaussian). So there is not a single answer to this question. In particular, we will show how to convert one formalism to another. Why does this equivalence matter? This allows you to mix and match techniques developed for the two frameworks. For example, after training a flow matching model, you can use either a stochastic or deterministic sampling method (in contrast to the common misunderstanding that flow matching is always deterministic).</p> <p>We will focus on the most commonly used flow matching formalism <d-cite key="lipman2022flow"></d-cite>, which is closely related to <d-cite key="liu2022flow,albergo2023stochastic"></d-cite>. Our purpose is not to downweigh the importance of either framework. In fact, both frameworks are important and derived from distinct theoretical perspectives. It is even more encouraging that they lead to the same algorithm in practice. Our goal is to help practitioners feel confident using the two frameworks interchangeably, while understanding the true degrees of freedom one has when tuning the algorithm—regardless of what it’s called.</p> <h2 id="overview">Overview</h2> <p>We start with a quick overview of the two frameworks.</p> <h3 id="diffusion-models">Diffusion models</h3> <p>A diffusion process gradually destroys an observed datapoint \(\bf{x}\) (such as an image) over time \(t\), by mixing the data with Gaussian noise. The noisy data at time \(t\) is given by a forward process: \(\begin{equation} {\bf z}_t = \alpha_t {\bf x} + \sigma_t {\boldsymbol \epsilon}, \;\mathrm{where} \; {\boldsymbol \epsilon} \sim \mathcal{N}(0, {\bf I}). \label{eq:forward} \end{equation}\) \(\alpha_t\) and \(\sigma_t\) define the <strong>noise schedule</strong>, such as the variance-preserving schedule, \(\alpha_t^2 + \sigma_t^2 = 1\). A useful notation is the log signal-to-noise ratio \(\lambda_t = \log(\alpha_t^2 / \sigma_t^2)\), which decreases as \(t\) increases from \(0\) (clean data) to \(1\) (Gaussian noise).</p> <p>To generate new samples, we can “reverse” the forward process: We initialize the sample \({\bf z}_1\) from a standard Gaussian. Given the sample \({\bf z}_t\) at time step \(t\), we predict what the clean sample might look like with a neural network (a.k.a. denoiser model) \(\hat{\bf x} = \hat{\bf x}({\bf z}_t; t)\), and then we project it back to a lower noise level \(s\) with the same forward transformation:</p> <p>\(\begin{eqnarray} {\bf z}_{s} &amp;=&amp; \alpha_{s} \hat{\bf x} + \sigma_{s} \hat{\boldsymbol \epsilon},\\ \end{eqnarray}\) where \(\hat{\boldsymbol \epsilon} = ({\bf z}_t - \alpha_t \hat{\bf x}) / \sigma_t\). (Alternatively we can train a neural network to predict the noise \(\hat{\boldsymbol \epsilon}\).) We keep alternating between predicting the clean data, and projecting it back to a lower noise level until we get the clean sample. This is the DDIM sampler <d-cite key="song2020denoising"></d-cite>. The randomness of samples only comes from the initial Gaussian sample, and the entire reverse process is deterministic. We will discuss the stochastic samplers later.</p> <h3 id="flow-matching">Flow matching</h3> <p>In flow Matching, the forward process is a linear interpolation between the data \({\bf x}\) and a noise term \(\boldsymbol \epsilon\): \(\begin{eqnarray} {\bf z}_t = (1-t) {\bf x} + t {\boldsymbol \epsilon}.\\ \end{eqnarray}\)</p> <p>This corresponds to the diffusion forward process if the noise is Gaussian and we use the schedule \(\alpha_t = 1-t, \sigma_t = t\).</p> <p>Using simple algebra, we can derive that \({\bf z}_t = {\bf z}_{s} + {\bf u} (t - s)\), where \({\bf u} = {\boldsymbol \epsilon} - {\bf x}\) is the “velocity”, “flow”, or “vector field”. Hence, to sample \({\bf z}_s\) given \({\bf z}_t\), for \(s &lt; t\), we reverse time and replace the vector field with our best guess \(\hat{\bf u} = \hat{\bf u}({\bf z}_t; t) = \hat{\boldsymbol \epsilon} - \hat{\bf x}\), represented by a neural network, to get</p> \[\begin{eqnarray} {\bf z}_{s} = {\bf z}_t + \hat{\bf u}(s - t).\\ \label{eq:flow_update} \end{eqnarray}\] <p>Starting from the sample \({\bf z}_1\) from a standart Gaussian, we keep getting \({\bf z}_s\) at a lower noise level than \({\bf z}_t\), until we obtain the clean sample.</p> <h3 id="comparison">Comparison</h3> <p>So far, we can already discern the similar essences in the two frameworks:</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> <p>1. <strong>Same forward process</strong>: if we assume that one end of flow matching is Gaussian, and the noise schedule of the diffusion model is in a particular form. </p> <p style="margin: 0;">2. <strong>"Similar" sampling processes</strong>: both follow an iterative update that involves a guess of the clean data at the current time step. (Spoiler: below we will show they are exactly the same!)</p> </div> <h2 id="sampling">Sampling</h2> <p>It is commonly thought that the two frameworks differ in how they generate samples: Flow matching sampling is deterministic with “straight” paths, while diffusion model sampling is stochastic and with “curved paths”. Below we clarify this misconception. We will focus on deterministic sampling first which is simpler; we discuss the stochastic case later on.</p> <p>Imagine you want to use your trained denoiser model to transform random noise into a datapoint. Recall that the DDIM update is given by \({\bf z}_{s} = \alpha_{s} \hat{\bf x} + \sigma_{s} \hat{\boldsymbol \epsilon}\). Interestingly, rearranging terms it can be expressed in the following formulation, with respect to several network outputs and reparametrizations:</p> \[\begin{equation} \tilde{\bf z}_{s} = \tilde{\bf z}_{t} + \mathrm{Network \; output} \cdot (\eta_s - \eta_t) \\ \end{equation}\] <table> <thead> <tr> <th style="text-align: left">Network Output</th> <th style="text-align: right">Reparametrization</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">\({\bf x}\)-prediction</td> <td style="text-align: right">\(\tilde{\bf z}_t = {\bf z}_t / \sigma_t\) and \(\eta_t = {\alpha_t}/{\sigma_t}\)</td> </tr> <tr> <td style="text-align: left">\({\boldsymbol \epsilon}\)-prediction</td> <td style="text-align: right">\(\tilde{\bf z}_t = {\bf z}_t / \alpha_t\) and \(\eta_t = {\sigma_t}/{\alpha_t}\)</td> </tr> <tr> <td style="text-align: left">\({\bf u}\)-flow matching vector field</td> <td style="text-align: right">\(\tilde{\bf z}_t = {\bf z}_t/(\alpha_t + \sigma_t)\) and \(\eta_t = {\sigma_t}/(\alpha_t + \sigma_t)\)</td> </tr> </tbody> </table> <p>Recall the flow matching update in Equation (4), look similar? In the last line, if we set \(\alpha_t = t\), \(\sigma_t = 1- t\), we have \(\tilde{\bf z}_t = {\bf z}_t\) and \(\eta_t = t\), so that we recover the flow matching update! More formally, the flow matching update can be considered the Euler integration of the underlying sampling ODE (i.e.,, \(\mathrm{d}\tilde{\bf z}_t = \mathrm{[Network \; output]}\cdot\mathrm{d}\eta_t\)), and</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> <p align="center" style="margin: 0;"><em>Diffusion with DDIM sampling == Flow matching sampling (Euler).</em></p> </div> <p>Some other comments on the DDIM sampler:</p> <ol> <li> <p>The DDIM sampler <em>analyically</em> integrates the underlying sampling ODE if the network output is a <em>constant</em> over time. Of course the network prediction is not constant, but it means the inaccuracy of DDIM sampler only comes from approximating the intractable integral of the network output (not from additional linear term of ${\bf z}_t$ as in the Euler sampler of probability flow ODE <d-cite key="song2020score"></d-cite>). This holds for all three network outputs.</p> </li> <li> <p>The DDIM update and final samples are invariant to a linear scaling applied to the noise schedule, as a scaling does not affect $\tilde{\bf z}_t$ and $\eta_t$.</p> </li> </ol> <p>To validate Claim 2, we present the results obtained using several noise schedules, each of which follows a flow-matching schedule with different scaling factors. At the left end, the scaling factor is \(1\) which is exactly the flow matching schedule, while at the right end, the scaling factor is \(1/[(1-t)^2 + t^2]\), which corresponds to a variance-preserving schedule (Feel free to change the slider). We see that DDIM (and flow matching sampler) always gives the same final samples, regardless of the scaling of the schedule. The paths bend in different ways as \({\bf z}_t\) (but not \(\tilde{\bf z}_t\)) is scale-dependent along the path. For the Euler sampler applied to the diffusion probabilty flow ODE introduced in <d-cite key="song2020score"></d-cite>, the scaling makes a true difference: we see that both the paths and the final samples change.</p> <div class="l-page"> <iframe src="/iclr25/assets/html/2025-04-28-distill-example/interactive_alpha_sigma.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p>Wait a second? It is often said that the flow matching results in <em>straight</em> paths, but in the above figure its sampling trajectories look <em>curved</em>.</p> <p>So why is flow matching said to result in straight sampling paths? If the model would be perfectly confident about the data point it is moving to, the path from noise to data will be a straight line with the flow matching schedule. Straight line ODEs would be great because it means that there is no integration error whatsoever. Unfortunately, the predictions are not for a single point. Instead they average over a larger distribution. And flowing <em>straight to a point != straight to a distribution</em>.</p> <p>In the interactive graph below, you can change the variance of the data distribution on the right hand side by the slider. Note how the variance preserving schedule is better (straighter paths) for wide distributions, while the flow matching schedule works better for narrow distributions.</p> <div class="l-page"> <iframe src="/iclr25/assets/html/2025-04-28-distill-example/interactive_vp_vs_flow.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p>Finding such straight paths for real-life datasets like images is of course much less straightforward. But the conclusion remains the same: The optimal integration method depends on the data distribution.</p> <p>Two important takeaways from deterministic sampling:</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> <p>1. DDIM is equivalent to the flow matching sampling, and is invariant to a linear scaling to the noise schedule. </p> <p style="margin: 0;">2. Flow matching schedule is only straight for a model predicting a single point. For realistic distributions other interpolations can give straighter paths.</p> </div> <h2 id="training">Training</h2> <p>Diffusion models <d-cite key="kingma2024understanding"></d-cite> are trained by estimating \(\hat{\bf x} = \hat{\bf x}({\bf z}_t; t)\), or alternatively \(\hat{\boldsymbol \epsilon} = \hat{\boldsymbol \epsilon}({\bf z}_t; t)\) with a neural net. Learning the model is done by minimizing a weighted mean squared error (MSE) loss: \(\begin{equation} \mathcal{L}(\mathbf{x}) = \mathbb{E}_{t \sim \mathcal{U}(0,1), \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})} \left[ \textcolor{green}{w(\lambda_t)} \cdot \frac{\mathrm{d}\lambda}{\mathrm{d}t} \cdot \lVert\hat{\boldsymbol \epsilon} - {\boldsymbol \epsilon}\rVert_2^2 \right], \end{equation}\) where \(\lambda_t\) is the log signal-to-noise ratio, and \(\textcolor{green}{w(\lambda_t)}\) is the <strong>weighting function</strong>, balancing the importance of the loss at different noise levels. The term \(\mathrm{d}\lambda / {\mathrm{d}t}\) in the training objective seems unnatural and in the literature is often merged with the weighting function. However, their separation helps <em>disentangle</em> the factors of noise schedule and weighting function clearly, and helps emphasize the more design choice: the weighting function.</p> <p>Flow matching also fits in the above training objective. Recall below is the conditional flow matching objective used by <d-cite key="lipman2022flow, liu2022flow"></d-cite> :</p> \[\begin{equation} \mathcal{L}_{\mathrm{CFM}}(\mathbf{x}) = \mathbb{E}_{t \sim \mathcal{U}(0,1), \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})} \left[ \lVert \hat{\bf u} - {\bf u} \rVert_2^2 \right] \end{equation}\] <p>Since \(\hat{\bf u}\) is a linear combination of \(\hat{\boldsymbol \epsilon}\) and \({\bf z}_t\), the CFM training objective can be rewritten as mean squared error on \({\boldsymbol \epsilon}\) with a specific weighting.</p> <h3 id="how-do-we-choose-what-the-network-should-output">How do we choose what the network should output?</h3> <p>Below we summarize several network outputs proposed in the literature, including a few versions used by diffusion models and the one used by flow matching. They can be derived from each other given the current data \({\bf z}_t\). One may see the training objective defined with different network outputs in different papers. From the perspective of training objective, they all correspond to having some additional weighting in front of the \({\boldsymbol \epsilon}\)-MSE that can be absorbed in the weighting function.</p> <table> <thead> <tr> <th style="text-align: left">Network Output</th> <th style="text-align: center">Formulation</th> <th style="text-align: right">MSE on Network Output</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">\({\boldsymbol \epsilon}\)-prediction</td> <td style="text-align: center">\(\hat{\boldsymbol \epsilon}\)</td> <td style="text-align: right">\(\lVert\hat{\boldsymbol{\epsilon}} - \boldsymbol{\epsilon}\rVert_2^2\)</td> </tr> <tr> <td style="text-align: left">\({\bf x}\)-prediction</td> <td style="text-align: center">\(\hat{\bf x} = ({\bf x}_t - \sigma_t \hat{\boldsymbol \epsilon}) / \alpha_t\)</td> <td style="text-align: right">\(\lVert\hat{\bf x} - {\bf x}\rVert_2^2 = e^{-\lambda} \lVert\hat{\boldsymbol \epsilon} - {\boldsymbol \epsilon}\rVert_2^2\)</td> </tr> <tr> <td style="text-align: left">\({\bf v}\)-prediction</td> <td style="text-align: center">\(\hat{\bf v} = \alpha_t \hat{\boldsymbol{\epsilon}} - \sigma_t \hat{\bf x}\)</td> <td style="text-align: right">\(\lVert\hat{\bf v} - {\bf v}\rVert_2^2 = \alpha_t^2(e^{-\lambda} + 1)^2 \lVert\hat{\boldsymbol \epsilon} - {\boldsymbol \epsilon}\rVert_2^2\)</td> </tr> <tr> <td style="text-align: left">\({\bf u}\)-flow matching vector field</td> <td style="text-align: center">\(\hat{\bf u} = \hat{\boldsymbol{\epsilon}} - \hat{\bf x}\)</td> <td style="text-align: right">\(\lVert\hat{\bf u} - {\bf u}\rVert_2^2 = (e^{-\lambda / 2} + 1)^2 \lVert\hat{\boldsymbol \epsilon} - {\boldsymbol \epsilon}\rVert_2^2\)</td> </tr> </tbody> </table> <p>In practice, however, the model output might make a difference. For example,</p> <ul> <li> <p>\({\boldsymbol \epsilon}\)-prediction can be problematic at high noise levels, because any error in \(\hat{\boldsymbol \epsilon}\) will get amplified in \(\hat{\bf x} = ({\bf x}_t - \sigma_t \hat{\boldsymbol \epsilon}) / \alpha_t\), as \(\alpha_t\) is close to 0. It means that small changes create a large loss under some weightings.</p> </li> <li> <p>Following the similar reason, \({\bf x}\)-prediction is problematic at low noise levels, because \(\hat{\bf x}\) is not informative, and the error gets amplified in \(\hat{\boldsymbol \epsilon}\).</p> </li> </ul> <p>Therefore, a heuristic is to choose a network output that is a combination of \({\bf x}\)- and \({\boldsymbol \epsilon}\)-predictions, which applies to the \({\bf v}\)-prediction and the flow matching vector field \({\bf u}\).</p> <h3 id="how-do-we-choose-the-weighting-term">How do we choose the weighting term?</h3> <p>The weighting is the most important part of the loss, it balances the importance of high frequency and low frequency components <d-cite key="dieleman2024spectral,kingma2024understanding"></d-cite>. This is important when modeling images, videos and audio, as certain high frequency components in those signals are not perceptible to humans, and thus it is better not to waste model capacity on them. Viewing losses via their weighting, one can derive that:</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> <p align="center" style="margin: 0;"><em>Flow matching weighting == diffusion weighting of ${\bf v}$-MSE loss + cosine noise schedule.</em></p> </div> <p>That is, the flow matching training objective is the same as a commonly used setting in diffusion models! See Appendix D.2-3 in <d-cite key="kingma2024understanding"></d-cite> for a detailed derivation. Below we plot several commonly used weighting functions in the literature, as a function of \(\lambda\).</p> <div class="m-page"> <iframe src="/iclr25/assets/html/2025-04-28-distill-example/weighting_functions.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p>The flow matching weighting (also \({\bf v}\)-MSE weighting) decreases exponentially as \(\lambda\) increases. Empirically we find another interesting connection: The stable diffusion 3 weighting, a reweighted version of flow matching, is very similar to the EDM weighting <d-cite key="karras2022elucidating"></d-cite> that is popular for diffusion models.</p> <h3 id="how-do-we-choose-the-noise-schedule">How do we choose the noise schedule?</h3> <p>A few remarks about training noise schedule:</p> <ol> <li>All noise schedules can be normalized as a variance-preserving schedule, with a linear scaling of \({\bf z}_t\) and an unscaling at the network input. The key defining property of a noise schedule is the log signal-to-noise ratio \(\lambda_t\).</li> <li>The training loss is <em>invariant</em> to the training noise schedule, since the loss fuction can be rewritten as \(\mathcal{L}(\mathbf{x}) = \int_{\lambda_{\min}}^{\lambda_{\max}} w(\lambda) \mathbb{E}_{\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})} \left[ \|\hat{\boldsymbol{\epsilon}} - \boldsymbol{\epsilon}\|_2^2 \right] \, d\lambda\), which is only related to the endpoints but not the schedule of \(\lambda_t\). However, \(\lambda_t\) might still affect the variance of the Monte Carlo estimator of the training loss. A few heuristics have been proposed in the literature to automatically adjust the noise schedules over the course of training. <a href="https://sander.ai/2024/06/14/noise-schedules.html#adaptive">This blog post</a> has a nice summary.</li> <li>One can choose completely different noise schedules for training and sampling, based on distinct heuristics: For training, it is desirable to have a noise schedule that minimizes the variance of the Monte Calor estimator, whereas for sampling the noise schedule is more related to the discretization error of the ODE / SDE sampling trajectories and the model curvature.</li> </ol> <h3 id="summary">Summary</h3> <p>In summary, we have the following conclusions for diffusion models / flow matching training:</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> <p>1. Weighting function <strong> is important for training</strong>. For perceptual signals, it balances the importance of different frequency components. It should be tuned based on data characteristics. </p> <p>2. Noise schedule <strong>is far less important to the training objective</strong> but affects the training efficiency.</p> <p style="margin: 0;">3. The network output proposed by flow matching nicely balances ${\bf x}$- and ${\epsilon}$-prediction, similar to ${\bf v}$-prediction.</p> </div> <h2 id="diving-deeper-into-samplers">Diving deeper into samplers</h2> <p>In this section, we discuss different kinds of samplers in more detail.</p> <h3 id="reflow-operator">Reflow operator</h3> <p>The Reflow operation in Flow Matching connects noise and data points in a straight line. One can obtain these (data, noise) pairs by running a deterministic sampler from noise. A model can then be trained to directly predict the data given the noise avoiding the need for sampling. In the diffusion literature the same approach was the one of the first distillation techniques <d-cite key="luhman2021knowledge"></d-cite>.</p> <h3 id="deterministic-sampler-vs-stochastic-sampler">Deterministic sampler vs. stochastic sampler</h3> <p>So far we have just discussed the deterministic sampler of diffusion models or flow matching. An alternative is to use stochastic samplers such as the DDPM sampler <d-cite key="ho2020denoising"></d-cite>.</p> <p>Performing one DDPM sampling step going from $\lambda_t$ to $\lambda_t + \Delta\lambda$ is exactly equivalent to performing one DDIM sampling step to $\lambda_t + 2\Delta\lambda$, and then renoising to $\lambda_t + \Delta\lambda$ by doing forward diffusion. The renoising by doing forward diffusion thus reverses exactly half the progress made by DDIM. To see this, let’s take a look at a 2D example. Starting from the same mixture of Gaussians distribution, we either perform a DDIM sampling step but reverse the sign of the update, or a forward diffusion step:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/particle_movement.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/particle_movement.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/particle_movement.gif-1400.webp"/> <img src="/iclr25/assets/img/2025-04-28-distill-example/particle_movement.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>For each individual sample, the two updates are very different: The reverse DDIM update consistently drags every sample away from the modes of the distribution, while the diffusion update is purely random. However, aggregating all samples together, the distributions after the updates are the same. Therefore if we perform the same DDIM sampling step (without reversing the sign of the update), followed by the forward diffusion step, the distribution will remain the same as the one before the two updates. </p> <p>The fraction of the DDIM step to undo by renoising is a hyperparameter which we are free to choose (doesn’t have to be , and which has been called the level of <em>churn</em> by <d-cite key="karras2022elucidating"></d-cite>. The effect of adding churn to our sampler is to diminish the effect on our final sample of our model predictions made early during sampling, and to increase the weight on later predictions. This is shown in the figure below</p> <div class="m-page"> <iframe src="/iclr25/assets/html/2025-04-28-distill-example/churn.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p>Here we ran different samplers for 100 sampling steps using a cosine noise schedule and \({\bf v}\)-prediction <d-cite key="salimansprogressive"></d-cite>. Ignoring nonlinear interactions, the final sample produced by the sampler can be written as a weighted sum of predictions \(\hat{\bf v}_t\) made during sampling and Gaussian noise \({\bf e}\): \({\bf z}_0 = \sum_t h_t \hat{\bf v}_t + \sum_t g_t {\bf e}\). The weights \(h_t\) of these predictions are shown on the y-axis for different diffusion times \(t\) shown on the x-axis. DDIM results in an equal weighting of \({\bf v}\)-predictions for this setting, as shown <d-cite key="salimansprogressive"></d-cite>, whereas DDPM puts more emphasis on predictions made towards the end of sampling. Also see <d-cite key="lu2022dpm"></d-cite> for analytic expressions of these weights in the \({\bf x}\) and \({\boldsymbol \epsilon}\) parameterizations.</p> <h2 id="sde-and-ode-perspective">SDE and ODE Perspective</h2> <p>We’ve observed the practical equivalence between diffusion models and flow matching algorithms. Here, we formally describe the equivalence of the forward process and sampling using ODE and SDE, as a completeness in theory and closing chapter. </p> <h3 id="diffusion-model">Diffusion Model</h3> <p>The forward process of diffusion models which gradually destroys a data over time can be described by the following stochastic differential equation (SDE):</p> \[\begin{equation} \mathrm{d} {\bf z}_t = f_t {\bf z}_t \mathrm{d} t + g_t \mathrm{d} {\bf z} , \end{equation}\] <p>where \(\mathrm{d} {\bf z}\) is an <em> infinitesimal Gaussian</em> (formally, a Brownian motion). $f_t$ and $g_t$ decide the noise schedule. The generative process is given by the reverse of the forward process, whose formula is given by</p> \[\begin{equation} \mathrm{d} {\bf z}_t = \left( f_t {\bf z}_t - \frac{1+ \eta_t^2}{2}g_t^2 \nabla \log p_t({\bf z_t}) \right) \mathrm{d} t + \eta_t g_t \mathrm{d} {\bf z} , \end{equation}\] <p>where $\nabla \log p_t$ is the <em>score</em> of the forward process. </p> <p>Note that we have introduced an additional parameter $\eta_t$ which controls the amount of stochasticity at inference time. This is related to the <em>churn</em> parameter introduced before. When discretizing the backward process we recover DDIM in the case $\eta_t = 0$ and DDPM in the case $\eta_t = 1$.</p> <h3 id="flow-matching-1">Flow Matching</h3> <p>The interpolation between \({\bf x}\) and \({\boldsymbol \epsilon}\) in flow matching can be described by the following ordinary differential equation (ODE):</p> \[\begin{equation} \mathrm{d}{\bf z}_t = {\bf u}_t \mathrm{d}t. \end{equation}\] <p>Assuming the interpolation is \({\bf z}_t = \alpha_t {\bf x} + \sigma_t {\boldsymbol \epsilon}\), then \({\bf u}_t = \dot{\alpha}_t {\bf x} + \dot{\sigma}_t {\boldsymbol \epsilon}\).</p> <p>The generative process is simply reverse the ODE in time. This is a specific case of <em>stochastic interpolation</em><d-cite key="liu2022flow,albergo2023stochastic"></d-cite>, in which case it can be generalized to an SDE:</p> <p>\(\begin{equation} \mathrm{d} {\bf z}_t = ({\bf u}_t - \frac{1}{2} \varepsilon_t^2 \nabla \log p_t({\bf z_t})) \mathrm{d} t + \varepsilon_t \mathrm{d} {\bf z}, \end{equation}\) where \(\varepsilon_t\) controls the amount of stochasticity at inference time.</p> <h3 id="equivalence-of-the-two-frameworks">Equivalence of the two frameworks</h3> <p>To summary, both frameworks are defined by three hyperparameters respectively: $f_t, g_t, \eta_t$ for diffusion, and $\alpha_t, \sigma_t, \varepsilon_t$ for flow matching. We can show the equivalence by deriving one set of hyperparameters from the other. From diffusion to flow matching:</p> \[\alpha_t = \exp\left(\int_0^t f_s \mathrm{d}s\right) , \quad \sigma_t = \left(\int_0^t g_s^2 \exp\left(-2\int_0^s f_u \mathrm{d}u\right) \mathrm{d} s\right)^{1/2} , \quad \varepsilon_t = \eta_t g_t .\] <p>From flow matching to diffusion:</p> \[f_t = \partial_t \log(\alpha_t) , \quad g_t = 2 \alpha_t \sigma_t \partial_t (\sigma_t / \alpha_t) , \quad \eta_t = \varepsilon_t / (2 \alpha_t \sigma_t \partial_t (\sigma_t / \alpha_t)) .\] <p>In summary, aside from training considerations and sampler selection, diffusion and Gaussian flow matching exhibit no fundamental differences.</p> <h2 id="closing-thoughts">Closing thoughts</h2> <p>If you’ve read this far, we hope we’ve convinced you that diffusion models and Gaussian flow matching are equivalent. When developing your own codebase or techniques, there’s no need to implement them separately under these two “distinct” frameworks. Similarly, if you’re working on one method, it’s likely unnecessary to reframe it within the context of the other. The key is to focus on the design choices that truly matter. With this equivalence in mind, improvements made to specific design choices in one framework will naturally benefit the other. We hope this perspective helps practitioners have more fun playing with these models, while fostering deeper connections between works in the literature.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Flow matching and diffusion models are two popular frameworks in generative modeling. Despite seeming similar, there is some confusion in the community about their exact connection. In this post we aim to clear up this confusion and show that diffusion models and Gaussian flow matching are the same -- Different model specifications lead to different noise schedules and loss weightings but correspond to the same generative model. That's great news, it means that you can use the two frameworks interchangeably.]]></summary></entry><entry><title type="html">Sample Blog Post (HTML version)</title><link href="https://diffusionflow.github.io/iclr25/blog/distill-example2/" rel="alternate" type="text/html" title="Sample Blog Post (HTML version)"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://diffusionflow.github.io/iclr25/blog/distill-example2</id><content type="html" xml:base="https://diffusionflow.github.io/iclr25/blog/distill-example2/"><![CDATA[<p> This is a sample blog post written in HTML (while the other <a href="/iclr25/blog/distill-example/">sample post</a> is written in Markdown). Authors have the choice to write in HTML or Markdown. While Markdown is easier to write, HTML gives you more control over the layout of your post. Furthermore, Markdown often interacts in unexpected ways with MathJax and other HTML widgets. If you are having trouble with Markdown, try writing in HTML instead. </p> <p> Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling. </p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code>$$</code>, like <code>$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code>$$</code> and place it as a separate paragraph. Here is an example: $$ \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) $$ </p> <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. You can display images from this repository using the following code:</p> <pre><code>{% include figure.html path="assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" %}</code></pre> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/iclr-1400.webp"/> <img src="/iclr25/assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p> To ensure that there are no namespace conflicts, you must save your asset to your unique directory `/assets/img/2025-04-28-[SUBMISSION NAME]` within your submission. </p> <p> Please avoid using the direct HTML method of embedding images; they may not be properly resized. Some below complex ways to load images (note the different styles of the shapes/shadows): </p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/9-1400.webp"/> <img src="/iclr25/assets/img/2025-04-28-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/iclr25/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/8-1400.webp"/> <img src="/iclr25/assets/img/2025-04-28-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/10-1400.webp"/> <img src="/iclr25/assets/img/2025-04-28-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/11-1400.webp"/> <img src="/iclr25/assets/img/2025-04-28-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/12-1400.webp"/> <img src="/iclr25/assets/img/2025-04-28-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/iclr25/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/iclr25/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3>Interactive Figures</h3> <p> Here's how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work. All that's required is for you to export your figure into HTML format, and make sure that the file exists in the `assets/html/[SUBMISSION NAME]/` directory in this repository's root directory. To embed it into any page, simply insert the following code anywhere into your page. </p> <pre><code>{% include [FIGURE_NAME].html %}</code></pre> <p> For example, the following code can be used to generate the figure underneath it. </p> <pre><code class="language-python">import pandas as pd
import plotly.express as px

df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')

fig = px.density_mapbox(
    df, lat='Latitude', lon='Longitude', z='Magnitude', radius=10,
    center=dict(lat=0, lon=180), zoom=0, mapbox_style="stamen-terrain")
fig.show()

fig.write_html('./assets/html/2025-04-28-distill-example/plotly_demo_1.html')
</code></pre> And then include it with the following: <pre><code class="language-html">&lt;div class="l-page"&gt;
  &lt;iframe src="{{ 'assets/html/2025-04-28-distill-example/plotly_demo_1.html' | relative_url }}" frameborder='0' scrolling='no' height="600px" width="100%"&gt;&lt;/iframe&gt;
&lt;/div&gt;
</code></pre> Voila! <div class="l-page"> <iframe src="/iclr25/assets/html/2025-04-28-distill-example/plotly_demo_1.html" frameborder='0' scrolling='no' height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p> Citations are then used in the article body with the <code>&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas. </p> <p> The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it. </p> <p> Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well - the authors are human and it's nice for them to have the community associate them with their work. </p> <h2 id="footnotes">Footnotes</h2> <p> Just wrap the text you would like to show up in a footnote in a <code>&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote> </p> <h2 id="code-blocks">Code Blocks</h2> <p> This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag as follows: </p> <pre><code>
{% highlight c++ linenos %}  <br/> code code code <br/> {% endhighlight %}

</code></pre> The keyword `linenos` triggers display of line numbers. You can try toggling it on or off yourself below: <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <h2 id="diagrams">Diagrams</h2> <p> This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="http://mermaid.js.org/">mermaid</a>, <a href="https://plantuml.com/">plantuml</a>, <a href="https://vega.github.io/vega-lite/">vega-lite</a>, etc. </p> <p> <b>Note</b>different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to the <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> README. </p> <p> <b>Note:</b> This is not supported for local rendering! </p> <p> The diagram below was generated by the following code: </p> <pre><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice->>John: Hello John, how are you?
    John-->>Alice: Great!
{% endmermaid %}

</code></pre> <div class='jekyll-diagrams diagrams mermaid'> <svg id="mermaid-1732356714220" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1732356714220 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1732356714220 .node circle,#mermaid-1732356714220 .node ellipse,#mermaid-1732356714220 .node polygon,#mermaid-1732356714220 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1732356714220 .node.clickable{cursor:pointer}#mermaid-1732356714220 .arrowheadPath{fill:#333}#mermaid-1732356714220 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1732356714220 .edgeLabel{background-color:#e8e8e8}#mermaid-1732356714220 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1732356714220 .cluster text{fill:#333}#mermaid-1732356714220 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1732356714220 .actor{stroke:#ccf;fill:#ececff}#mermaid-1732356714220 text.actor{fill:#000;stroke:none}#mermaid-1732356714220 .actor-line{stroke:grey}#mermaid-1732356714220 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1732356714220 .messageLine0,#mermaid-1732356714220 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1732356714220 #arrowhead{fill:#333}#mermaid-1732356714220 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1732356714220 .messageText{fill:#333;stroke:none}#mermaid-1732356714220 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1732356714220 .labelText,#mermaid-1732356714220 .loopText{fill:#000;stroke:none}#mermaid-1732356714220 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1732356714220 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1732356714220 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1732356714220 .section{stroke:none;opacity:.2}#mermaid-1732356714220 .section0{fill:rgba(102,102,255,.49)}#mermaid-1732356714220 .section2{fill:#fff400}#mermaid-1732356714220 .section1,#mermaid-1732356714220 .section3{fill:#fff;opacity:.2}#mermaid-1732356714220 .sectionTitle0,#mermaid-1732356714220 .sectionTitle1,#mermaid-1732356714220 .sectionTitle2,#mermaid-1732356714220 .sectionTitle3{fill:#333}#mermaid-1732356714220 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1732356714220 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1732356714220 .grid path{stroke-width:0}#mermaid-1732356714220 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1732356714220 .task{stroke-width:2}#mermaid-1732356714220 .taskText{text-anchor:middle;font-size:11px}#mermaid-1732356714220 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1732356714220 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1732356714220 .taskText0,#mermaid-1732356714220 .taskText1,#mermaid-1732356714220 .taskText2,#mermaid-1732356714220 .taskText3{fill:#fff}#mermaid-1732356714220 .task0,#mermaid-1732356714220 .task1,#mermaid-1732356714220 .task2,#mermaid-1732356714220 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1732356714220 .taskTextOutside0,#mermaid-1732356714220 .taskTextOutside1,#mermaid-1732356714220 .taskTextOutside2,#mermaid-1732356714220 .taskTextOutside3{fill:#000}#mermaid-1732356714220 .active0,#mermaid-1732356714220 .active1,#mermaid-1732356714220 .active2,#mermaid-1732356714220 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1732356714220 .activeText0,#mermaid-1732356714220 .activeText1,#mermaid-1732356714220 .activeText2,#mermaid-1732356714220 .activeText3{fill:#000!important}#mermaid-1732356714220 .done0,#mermaid-1732356714220 .done1,#mermaid-1732356714220 .done2,#mermaid-1732356714220 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1732356714220 .doneText0,#mermaid-1732356714220 .doneText1,#mermaid-1732356714220 .doneText2,#mermaid-1732356714220 .doneText3{fill:#000!important}#mermaid-1732356714220 .crit0,#mermaid-1732356714220 .crit1,#mermaid-1732356714220 .crit2,#mermaid-1732356714220 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1732356714220 .activeCrit0,#mermaid-1732356714220 .activeCrit1,#mermaid-1732356714220 .activeCrit2,#mermaid-1732356714220 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1732356714220 .doneCrit0,#mermaid-1732356714220 .doneCrit1,#mermaid-1732356714220 .doneCrit2,#mermaid-1732356714220 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1732356714220 .activeCritText0,#mermaid-1732356714220 .activeCritText1,#mermaid-1732356714220 .activeCritText2,#mermaid-1732356714220 .activeCritText3,#mermaid-1732356714220 .doneCritText0,#mermaid-1732356714220 .doneCritText1,#mermaid-1732356714220 .doneCritText2,#mermaid-1732356714220 .doneCritText3{fill:#000!important}#mermaid-1732356714220 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1732356714220 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1732356714220 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1732356714220 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1732356714220 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1732356714220 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1732356714220 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1732356714220 #compositionEnd,#mermaid-1732356714220 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1732356714220 #aggregationEnd,#mermaid-1732356714220 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1732356714220 #dependencyEnd,#mermaid-1732356714220 #dependencyStart,#mermaid-1732356714220 #extensionEnd,#mermaid-1732356714220 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1732356714220 .branch-label,#mermaid-1732356714220 .commit-id,#mermaid-1732356714220 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1732356714220{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div> <h2 id="tweets">Tweets</h2> <p> An example of displaying a tweet: <div class='jekyll-twitter-plugin'><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </p> <p> An example of pulling from a timeline: <div class='jekyll-twitter-plugin'><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </p> <p> For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a> </p> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <h2 id="layouts">Layouts</h2> The main text column is referred to as the body. It's the assumed layout of any direct descendants of the `d-article` element. <div class="fake-img l-body"> <p>.l-body</p> </div> For images you want to display a little larger, try `.l-page`: <div class="fake-img l-page"> <p>.l-page</p> </div> All of these have an outset variant if you want to poke out from the body text a little bit. For instance: <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> Occasionally you'll want to use the full browser width. For this, use `.l-screen`. You can also inset the element a little from the edge of the browser by using the inset variant. <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of `.l-body`-sized text except on mobile screen sizes. <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <h2 id="other-typography">Other Typography?</h2> <p> Emphasis, aka italics, with the <code>&lt;i&gt;&lt;/i&gt;</code> tag <i>emphasis</i>. </p> <p> Strong emphasis, aka bold, with <code>&lt;b&gt;&lt;/b&gt;</code> tag <b>bold</b>. </p> <p> Strikethrough ca be accomplished with the <code>&lt;s&gt;&lt;/s&gt;</code> tag. <s>Scratch this.</s> </p> <ul> <li>First ordered list item</li> <li>Another item</li> <ol> <li>Unordered sub-list. </li> </ol> <li>And another item.</li> </ul> <p> For code, the language can be specified in the class. For example, use <q>language-javascript</q> for Javascript and <q>language-python</q> for Python code. </p> <pre><code class="language-javascript">var s = "JavaScript syntax highlighting";
  alert(s);</code></pre> <pre><code class="language-python">s = "Python syntax highlighting"
  print(s)</code></pre> <pre><code class="language-python">No language indicated, so no syntax highlighting.</code></pre> <p> A table can be created with the <code>&lt;table&gt;</code> element. Below is an example </p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p> <blockquote>Blockquotes can be defined with the &gt;blockquote&lt; tag.</blockquote> </p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry></feed>